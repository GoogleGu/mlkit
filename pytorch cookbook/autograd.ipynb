{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd.function import Function"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自动求导\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 扩展自动求导函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个乘以常数的操作(输入参数是张量)\n",
    "# 方法必须是静态方法，所以要加上@staticmethod \n",
    "class MulConstant(Function):\n",
    "    @staticmethod \n",
    "    def forward(ctx, tensor, constant):\n",
    "        # ctx 用来保存信息这里类似self，并且ctx的属性可以在backward中调用\n",
    "        ctx.constant=constant\n",
    "        return tensor *constant\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # 返回的参数要与输入的参数一样.\n",
    "        # 第一个输入为3x3的张量，第二个为一个常数\n",
    "        # 常数的梯度必须是 None.\n",
    "        return grad_output * ctx.constant, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "a:tensor([[0.6610, 0.5404, 0.3890],\n        [0.9556, 0.1115, 0.6141],\n        [0.1649, 0.4090, 0.9512]], requires_grad=True)\nb:tensor([[3.3050, 2.7022, 1.9450],\n        [4.7780, 0.5574, 3.0704],\n        [0.8244, 2.0450, 4.7560]], grad_fn=<MulConstantBackward>)\n"
    }
   ],
   "source": [
    "a=torch.rand(3,3,requires_grad=True)\n",
    "b=MulConstant.apply(a,5)\n",
    "print(\"a:\"+str(a))\n",
    "print(\"b:\"+str(b)) # b为a的元素乘以5\n",
    "b.backward(torch.ones_like(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[5., 5., 5.],\n        [5., 5., 5.],\n        [5., 5., 5.]])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[5., 5., 5.],\n        [5., 5., 5.],\n        [5., 5., 5.]])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 与原生的乘法运算比较，二者行为应当一致\n",
    "a = torch.rand(3,3,requires_grad=True)\n",
    "b = a * 5\n",
    "b.backward(torch.ones_like(a))\n",
    "a.grad"
   ]
  }
 ]
}